<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="style_project_page.css" media="screen"/>
<link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="https://unpkg.com/@glidejs/glide/dist/css/glide.core.min.css">


<html lang="en">
<head>
	<title>SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements</title>
    <link rel="icon" type="image/png" href="projects/SCALE/figures/icon.png">

	<meta property="og:image" content="https://qianlim.github.io/projects/SCALE/figures/teaser_1200x630.png"/>
	<meta property="og:title" content="[CVPR 2021] SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements" />
	<meta property="og:description" content="Modeling pose-dependent shapes of clothed humans explicitly with hundreds
     of articulated surface elements: the clothing deforms naturally even in the presence of topological change!" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="twitter:card" content="summary_large_image" />
    <meta property="twitter:title" content="[CVPR 2021] SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements" />
    <meta property="twitter:description" content="Modeling pose-dependent shapes of clothed humans explicitly with hundreds
    of articulated surface elements: the clothing deforms naturally even in the presence of topological change!" />
    <meta property="twitter:image" content="https://qianlim.github.io/projects/SCALE/figures/teaser_1200x630.png" />
    <meta property="twitter:image:alt" content="SCALE, Ma et al., CVPR 2021" />


    <!-- Add your Google Analytics tag here -->
    <!-- <script async
            src=""></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <h1 class="project-title">
        SCALE: Modeling Clothed Humans with a <br /> Surface Codec of Articulated Local Elements
    </h1>

    <div class="conference">
        In CVPR 2021
    </div>

    <br><br>


    <div class="authors">
        <a href=https://qianlim.github.io/>
            Qianli Ma <sup>1,2</sup>
        </a>
        <a href=http://www-scf.usc.edu/~saitos/>
            Shunsuke Saito <sup>1</sup>
        </a>
        <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>
            Jinlong Yang <sup>1</sup>
        </a>
        <a href=https://vlg.inf.ethz.ch/people/person-detail.siyutang.html/>
            Siyu Tang <sup>2</sup>
        </a>
        <a href=https://ps.is.tuebingen.mpg.de/person/black/>
        Michael J. Black <sup>1</sup>
        </a>
    </div>
    <br>

    <div class="affiliations">
        <span><sup>1</sup> Max Planck Institute for Intelligent Systems, Tübingen</span></br>
        <span><sup>2</sup> ETH Zürich</span> </br>
    </div>
    <br><br>

    <div class="project-icons">
        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_SCALE_Modeling_Clothed_Humans_with_a_Surface_Codec_of_Articulated_CVPR_2021_paper.pdf">
            <i class="fa fa-file-pdf-o"></i> <br/>
            Paper
        </a>
        <a href="https://github.com/qianlim/SCALE">
            <i class="fa fa-github"></i> <br/>
            Code <br/>
        </a>
        <a href="https://youtu.be/-EvWqFCUb7U">
            <i class="fa fa-youtube-play"></i> <br/>
            Video
        </a>
        <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/650/SCALE_poster_CVPR_final_compressed.pdf">
            <i class="fa fa-newspaper-o"></i> <br/>
            Poster
        </a>
    </div>

    <div class="teaser">
        <img src="./projects/SCALE/figures/SCALE_teaser_new.png" alt="Teaser figure."/>
        <br>
        <p style="width: 90%; text-align: center;">
            SCALE models 3D clothed humans with hundreds of <b>articulated surface elements</b>, 
            resulting in avatars with <b>realistic clothing</b> that deforms naturally even in 
            the presence of <b>topological change</b>.
        </p>
    </div>

    <br><br>
    
    <hr>
    <h1>Abstract</h1>

    <p style="width: 85%">
        Learning to model and reconstruct humans in clothing is challenging due to articulation, non-rigid deformation, and varying clothing types and topologies. 
        To enable learning, the choice of representation is the key.
        Recent work uses neural networks to parameterize local surface elements.
        This approach captures locally coherent geometry and non-planar details, can deal with varying topology, and does not require registered training data.
        However, naively using such methods to model 3D clothed humans fails to capture fine-grained local deformations and generalizes poorly. 
        To address this, we present three key innovations: 
        First, we deform surface elements based on a human body model such that large-scale deformations caused by articulation are explicitly separated from topological changes and local clothing deformations. 
        Second, we address the limitations of existing neural surface elements by regressing local geometry from local features, significantly improving the expressiveness.
        Third, we learn a pose embedding on a 2D parameterization space that encodes posed body geometry, improving generalization to unseen poses by reducing non-local spurious correlations.
        We demonstrate the efficacy of our surface representation by learning models of complex clothing from point clouds. The clothing can change topology and deviate from the topology of the body. Once learned, we can  animate previously unseen motions, producing high-quality point clouds, from which we generate realistic images with neural rendering.
        We assess the importance of each technical contribution and show that our approach outperforms the state-of-the-art methods in terms of reconstruction accuracy and inference time.
        The code is available for research purposes.</p>
    </p>

    <br><br>
    <hr>
    <h1> Animated Results</h1>
    <table style="width:90%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;text-align:center;">
        <tr>
          <td width="33%">Predicted Surface Elements</td>
          <td width="33%">Predicted Point Color / Normal</td>
          <td width="33%">Neural Rendered</td>
        </tr>
      </table>
      <br>
    <img src="projects/SCALE/figures/gifs/blazerlong_ballet3_combined.gif" style="width:90%;" height="auto">
    <img src="projects/SCALE/figures/gifs/blazerlong_swim_combined.gif" style="width:90%;" height="auto">
    <img src="projects/SCALE/figures/gifs/skirt_club_trial2_combined.gif" style="width:90%;" height="auto">
    <img src="projects/SCALE/figures/gifs/skirt_frisbee_trial1_combined.gif" style="width:90%;" height="auto">
    
    <br><br>
    <hr>
    <h1>Video</h1>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/-EvWqFCUb7U" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <br><br>


    <hr>
    <h1>Paper</h1>
       <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_SCALE_Modeling_Clothed_Humans_with_a_Surface_Codec_of_Articulated_CVPR_2021_paper.pdf"><img src="projects/SCALE/figures/paper_snapshot_1x8.png" style="width:85%;" height="auto"></a>

       <div class="paper-info">
       <br>
       <span style="font-size: 14pt; font-weight: bold;">SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements</span><br>
       <span style="font-size: 14pt;"> Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang and Michael J. Black. </span>  <br>
       <span style="font-size: 14pt;">In CVPR 2021</span> <br>
       <span style="font-size: 14pt;"><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_SCALE_Modeling_Clothed_Humans_with_a_Surface_Codec_of_Articulated_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[PDF]</a>&nbsp; <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Ma_SCALE_Modeling_Clothed_CVPR_2021_supplemental.pdf" target="_blank" rel="noopener">[Supp]</a>&nbsp;
       <a href="https://arxiv.org/abs/2104.07660" target="_blank" rel="noopener">[arXiv]<br /></a>
    
       <pre><code>@inproceedings{Ma:CVPR:2021,
title = {{SCALE}: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements},
author = {Ma, Qianli and Saito, Shunsuke and Yang, Jinlong and Tang, Siyu and Black, Michael J.},
booktitle = {Proceedings IEEE/CVF Conf.~on Computer Vision and Pattern Recognition (CVPR)},
month = jun,
year = {2021},
month_numeric = {6}}
</code></pre>
</div>

    <br><br>

    <hr>
    <h1>Related Projects</h1>
    <p style="width: 85%;text-align:left; ">
    <b><a href="https://scanimate.is.tue.mpg.de/">SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks (CVPR 2021)</a></b> <br>
    <i>Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J. Black</i><br>
    While SCALE is an <i>explicit</i> representation, our SCANimate provides an <i>implicit</i> solution:
    cycle-consistent implicit skinning fields + locally pose-aware implicit function = 
    a fully animatable avatar with implicit surface from raw scans without surface registration!<br><br>

    <b><a href="https://cape.is.tue.mpg.de/">Learning to Dress 3D People in Generative Clothing (CVPR 2020)</a></b> <br>
    <i>Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, Michael J. Black</i><br>
    CAPE &mdash; a generative model and a large-scale dataset for 3D clothed human meshes in varied poses and garment types. 
    We trained SCALE using the <a href="https://cape.is.tue.mpg.de/dataset">CAPE dataset</a>, check it out!

    </p>

    <br><br>
    <hr>
    <h1>Acknowledgements</h1>

    <p style="width: 85%;">
        We thank Sergey Prokudin for insightful discussions and providing
        <a href="https://arxiv.org/abs/2008.06872">SMPLpix neural rendering</a> results. <br>

        Q. Ma acknowledges the funding by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
        and the support from the <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a>.<br>
        
        The webpage template is adapted from those of 
        <a href="https://sorderender.github.io/">RADAR</a> and
        <a href="https://paschalidoud.github.io/neural_parts">Neural Parts</a>.
    
    </p>

    <br><br>
</div>

</body>

</html>